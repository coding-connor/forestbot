{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are crucial in today's natural language processing (NLP) landscape, and their importance can be summarized as follows:\n",
      "\n",
      "1. **Real-time Applications**: Fast language models enable real-time language understanding and generation capabilities, which are essential for applications like:\n",
      "\t* Chatbots and virtual assistants, which require rapid response times to maintain user engagement.\n",
      "\t* Sentiment analysis and opinion mining, where timely insights are critical for business decision-making.\n",
      "\t* Query-based systems, such as search engines, that rely on quick responses to user queries.\n",
      "2. **Low-Latency Inference**: Fast language models can perform inference (e.g., language translation, text classification) rapidly, reducing the time it takes to process and respond to user input. This is particularly important in:\n",
      "\t* Real-time language translation systems, which require instant translation to facilitate communication.\n",
      "\t* High-volume text classification tasks, where fast processing is necessary to handle large datasets.\n",
      "3. **Efficient Resource Utilization**: Fast language models can operate on computation-constrained devices, making them suitable for:\n",
      "\t* Edge AI applications, where models need to run on resource-scarce devices like smartphones or IoT devices.\n",
      "\t* Serverless computing architectures, which prioritize efficiency and cost-effectiveness.\n",
      "4. **Scalability**: Fast language models can handle large volumes of data and traffic, making them suitable for:\n",
      "\t* High-traffic websites and applications that require rapid language processing.\n",
      "\t* Large-scale NLP tasks, such as language modeling, where massive datasets need to be processed efficiently.\n",
      "5. **Cost-Effectiveness**: Fast language models can reduce computational costs by minimizing the time and resources needed for language processing, leading to:\n",
      "\t* Cost savings on cloud computing resources and infrastructure.\n",
      "\t* Increased profitability for businesses that rely on NLP capabilities.\n",
      "6. **Improved User Experience**: Fast language models can lead to:\n",
      "\t* Enhanced user engagement, as users receive rapid responses and can interact more naturally with language-based systems.\n",
      "\t* Increased customer satisfaction, as language-based systems respond quickly and accurately to user queries.\n",
      "7. **Advancements in AI Research**: Fast language models can accelerate AI research by enabling:\n",
      "\t* Faster prototyping and testing of NLP models, leading to accelerated innovation.\n",
      "\t* Exploration of new AI applications and use cases that were previously not feasible due to computational constraints.\n",
      "\n",
      "In summary, fast language models are essential for various applications, enabling real-time language understanding, efficient resource utilization, scalability, and cost-effectiveness, while improving user experiences and driving advancements in AI research.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    #    model=\"mixtral-8x7b-32768\",\n",
    "    model=\"llama3-70b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GraphDocument(nodes=[Node(id='Llama', type='Animal'), Node(id='Peru', type='Location')], relationships=[Relationship(source=Node(id='Llama', type='Animal'), target=Node(id='Peru', type='Location'), type='LIVES_IN')], source=Document(page_content='Llama is a cool animal that leaves in Peru'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.graph_transformers.llm import LLMGraphTransformer\n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "\n",
    "from langchain_community.docstore.document import Document\n",
    "\n",
    "\n",
    "# Initialisation de ChatGrog avec les paramètres souhaités\n",
    "llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\")\n",
    "\n",
    "# Initialisation de LLMGraphTransformer avec le modèle de langage\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "\n",
    "# Conversion du document en graph document\n",
    "\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(\n",
    "    [Document(page_content=\"Llama is a cool animal that leaves in Peru\")]\n",
    ")\n",
    "# Affichage du graph document\n",
    "print(graph_documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forestbot-QnTDFiWY-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
